[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Математичне моделювання в R",
    "section": "",
    "text": "Матеріали підготовлені для читання курсу “Вступ до прикладного математичного моделювання в R” [08.435] для студентів 3-го курсу, спеціальності економічна кібернетика.\n\n\nНавчальна дисципліна спрямована на вивчення основ прикладного математичного моделювання з використаням різних видів моделей та підходів до їх побудови, а також удосокналенню процесів роботи з даними та моделями на основі мови R.\nМісце навчальної дисципліни у підготовці здобувачів: програмні результати дисципліни використовуються під час вивчення таких навчальних дисциплін: “Математичні методи та моделі в аналізі великих даних”, “Економічна кібернетика”, “Дослідження операцій”, “Інтелектуальні технології моделювання у прийнятті рішень”. Закріплення на практиці здобутих програмних результатів відбувається під час проходження Навчальної практики з курсу «Економіко-математичне моделювання».\n\n\n\nМета навчальної дисципліни – формування у студентів теоретичних знань та практичних навичок використання мови програмування R для побудови, навчання та оцінки якості математичних моделей на основі регресії, класифікації, кластеризації та асоціативних правил.\n\n\n\nВикладач та слухач цього курсу, як очікується, повинні дотримуватися Кодексу академічної доброчесності університету:\n\nбудь-яка робота, подана здобувачем протягом курсу, має бути його власною роботою здобувача; не вдаватися до кроків, що можуть нечесно покращити Ваші результати чи погіршити/покращити результати інших здобувачів;\nякщо буде виявлено ознаки плагіату або іншої недобросовісної академічної поведінки, то студент буде позбавлений можливості отримати передбачені бали за завдання;\nне публікувати у відкритому доступі відповіді на запитання, що використовуються в рамках курсу для оцінювання знань здобувачів;\nпід час фінальних видів контролю необхідно працювати самостійно; не дозволяється говорити або обговорювати, а також не можна копіювати документи, використовувати електронні засоби отримання інформації.\n\nПорушення академічної доброчесності під час виконання контрольних завдань призведе до втрати балів або вживання заходів, які передбачені Кодексу академічної доброчесності НаУОА.\n\n\n\n\n\n\n\nМатеріали курсу створені з використанням ряду технологій:\n\nR Language - безкоштована мова програмування для виконання досліджень у сфері статистики, машинного навчання та візуалізацї результатів.\nQuarto Book - система для публікації наукових та технічних текстів з відкритим кодом (R/Python/Julia/Observable).\nJupyterLab - середовище розробки на основі Jupyter Notebook. JupyterLab є розширеним веб-інтерфейсом для роботи з ноутбуками.\nGit/Github - система контролю версій та, відповідно, сервіс для організації зберігання коду, а також публікації статичних сторінок."
  },
  {
    "objectID": "010-intro.html",
    "href": "010-intro.html",
    "title": "ТЕМА 1. ВСТУП ДО КУРСУ",
    "section": "",
    "text": "Мета заняття:\nРозглянути основні поняття, що стосуються математичного моделювання та базові категорії машинного навчання.\n\n\nЦе вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "020-feature-engeneering.html",
    "href": "020-feature-engeneering.html",
    "title": "ТЕМА 2. КОНСТРУЮВАННЯ ОЗНАК",
    "section": "",
    "text": "У даній темі розкрито основні принципи інженерії фіч та методів трансформації даних для підвищення якості, швидкості побудови математичних моделей.\n\n\n<p>Це вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>.</p>"
  },
  {
    "objectID": "023-feature-engeneering-woe-binning.html",
    "href": "023-feature-engeneering-woe-binning.html",
    "title": "1  Woe Binning",
    "section": "",
    "text": "Курс: “Матетичне моделювання в R”\n\n\n# install.packages(\"openxlsx\")\n\n\nSys.setlocale(\"LC_CTYPE\", \"ukrainian\")\noptions(warn = -1)\n\n'Ukrainian_Ukraine.1251'\n\n\n\n\nДжерело: https://github.com/gastonstat/CreditScoring/blob/master/CleanCreditScoring.csv\nЗавантажимо дані:\n\nlibrary(openxlsx)\ndata <- openxlsx::read.xlsx(\"data/CreditScoring.xlsx\", sheet = 1, startRow = 1, colNames = TRUE, rowNames = FALSE)\nstr(data)\n\n'data.frame':   4446 obs. of  16 variables:\n $ Status   : chr  \"good\" \"good\" \"bad\" \"good\" ...\n $ Seniority: num  9 17 10 0 0 1 29 9 0 0 ...\n $ Home     : chr  \"rent\" \"rent\" \"owner\" \"rent\" ...\n $ Time     : num  60 60 36 60 36 60 60 12 60 48 ...\n $ Age      : num  30 58 46 24 26 36 44 27 32 41 ...\n $ Marital  : chr  \"married\" \"widow\" \"married\" \"single\" ...\n $ Records  : chr  \"no_rec\" \"no_rec\" \"yes_rec\" \"no_rec\" ...\n $ Job      : chr  \"freelance\" \"fixed\" \"freelance\" \"fixed\" ...\n $ Expenses : num  73 48 90 63 46 75 75 35 90 90 ...\n $ Income   : num  129 131 200 182 107 214 125 80 107 80 ...\n $ Assets   : num  0 0 3000 2500 0 3500 10000 0 15000 0 ...\n $ Debt     : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Amount   : num  800 1000 2000 900 310 650 1600 200 1200 1200 ...\n $ Price    : num  846 1658 2985 1325 910 ...\n $ Finrat   : num  94.6 60.3 67 67.9 34.1 ...\n $ Savings  : num  4.2 4.98 1.98 7.93 7.08 ...\n\n\nОпишемо дані:\n\nStatus - credit status (Target)\nSeniority job seniority (years)\nHome type of home ownership\nTime time of requested loan\nAge client’s age\nMarital marital status\nRecords existance of records\nJob type of job\nExpenses amount of expenses\nIncome amount of income\nAssets amount of assets\nDebt amount of debt\nAmount amount requested of loan\nPrice price of good\n\n\n\n\n\n\n\nСтворимо дата-фрейм для зберігання інформації про групи змінної Home (як приклад біннігу категоріального показника):\n\nhome_groups <- data.frame(Group = unique(data$Home), \n                          Good = c(0), Bad = c(0), \n                          GoodP = c(0), BadP = c(0),\n                          WOE = c(0), IV = c(0))\nhome_groups\n\n\n\nA data.frame: 6 × 7\n\n    GroupGoodBadGoodPBadPWOEIV\n    <chr><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    rent   000000\n    owner  000000\n    parents000000\n    priv   000000\n    other  000000\n    ignore 000000\n\n\n\n\nПереглянемо можливі варіанти показника Status, який є залежною бінарною змінною поточної задачі:\n\nlibrary(gmodels)\nCrossTable(data$Status)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4446 \n\n \n          |       bad |      good | \n          |-----------|-----------|\n          |      1249 |      3197 | \n          |     0.281 |     0.719 | \n          |-----------|-----------|\n\n\n\n \n\n\nСформуємо групи та обчислимо значення по кожній групі:\n\nfor(i in 1:nrow(home_groups)) {\n  \n  group <- home_groups$Group[i]\n  \n  home_groups$Good[i] <- nrow(data[data$Home == group & data$Status == \"good\", ])\n  home_groups$Bad[i] <- nrow(data[data$Home == group & data$Status == \"bad\", ])\n  \n  home_groups$GoodP[i] <- home_groups$Good[i]/nrow(data[data$Status == \"good\", ])\n  home_groups$BadP[i] <- home_groups$Bad[i]/nrow(data[data$Status == \"bad\", ])\n  \n  home_groups$WOE[i] <- log( home_groups$GoodP[i] / home_groups$BadP[i])\n  home_groups$IV[i] <- (home_groups$GoodP[i] - home_groups$BadP[i])*home_groups$WOE[i]\n}\n\nhome_groups\n\n\n\nA data.frame: 6 × 7\n\n    GroupGoodBadGoodPBadPWOEIV\n    <chr><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    rent    5853880.1829840480.310648519-0.529263130.067568098\n    owner  17163900.5367532060.312249800 0.541734900.121621331\n    parents 5502320.1720362840.185748599-0.076688730.001051580\n    priv    162 840.0506725050.067253803-0.283090100.004694001\n    other   1731460.0541132310.116893515-0.770184670.048352412\n    ignore   11  90.0034407260.007205765-0.739198940.002783113\n\n\n\n\nПереглянемо сумарний IV:\n\nhome_iv <- sum(home_groups$IV)\nhome_iv\n\n0.246070534510342\n\n\nВізуалізуємо групи:\n\nbarplot(home_groups$WOE, \n        col=\"brown\", \n        names.arg=c(as.character(home_groups$Group)), \n        xlab=\"Group\",\n        ylab=\"WOE\"\n)\n\n\n\n\nСтворимо датафрейм для нових WOE-даних:\n\nnew_df <- data.frame(Status = data$Status, Home = data$Home, HomeWoe = c(0))\n\nЗамінимо значення на WOE:\n\nfor(i in 1:nrow(home_groups)) {\n  group <- home_groups$Group[i]\n  woe <- home_groups$WOE[i]  \n  new_df[new_df$Home == group, ]$HomeWoe <- woe\n}\n\ntail(new_df)\n\n\n\nA data.frame: 6 × 3\n\n    StatusHomeHomeWoe\n    <chr><chr><dbl>\n\n\n    4441bad other-0.7701847\n    4442bad rent -0.5292631\n    4443goodowner 0.5417349\n    4444bad owner 0.5417349\n    4445goodrent -0.5292631\n    4446goodowner 0.5417349\n\n\n\n\n\n\n\n\nОбрахуємо приклад числовиго показника (на прикладі Age):\n\nmin_age <- min(data$Age)\nmax_age <- max(data$Age)\n\nstep <- round(max_age - min_age)/10\n\n\nage_groups <- data.frame(Min = seq(min_age, max_age-step, step), \n                         Max = seq(min_age + step, max_age, step), \n                          Good = c(0), Bad = c(0), \n                          GoodP = c(0), BadP = c(0),\n                          WOE = c(0), IV = c(0))\n\nage_groups\n\n\n\nA data.frame: 10 × 8\n\n    MinMaxGoodBadGoodPBadPWOEIV\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1823000000\n    2328000000\n    2833000000\n    3338000000\n    3843000000\n    4348000000\n    4853000000\n    5358000000\n    5863000000\n    6368000000\n\n\n\n\nСформуємо групи:\n\nfor(i in 1:nrow(age_groups)) {\n  \n  min <- age_groups$Min[i]\n  max<- age_groups$Max[i]\n  \n  age_groups$Good[i] <- nrow(data[data$Age >= min & data$Age < max & data$Status == \"good\", ])\n  age_groups$Bad[i] <- nrow(data[data$Age >= min & data$Age < max & data$Status == \"bad\", ])\n  \n  if(i == nrow(age_groups)) {\n    age_groups$Good[i] <- age_groups$Good[i] + nrow(data[data$Age == max & data$Status == \"good\", ])\n    age_groups$Bad[i] <- age_groups$Bad[i] + nrow(data[data$Age == max & data$Status == \"bad\", ])\n  }\n  \n  age_groups$GoodP[i] <- age_groups$Good[i]/nrow(data[data$Status == \"good\", ])\n  age_groups$BadP[i] <- age_groups$Bad[i]/nrow(data[data$Status == \"bad\", ])\n  \n  age_groups$WOE[i] <- log( age_groups$GoodP[i] / age_groups$BadP[i])\n  \n  age_groups$IV[i] <- (age_groups$GoodP[i] - age_groups$BadP[i]) * age_groups$WOE[i]\n}\n\nage_groups\n\n\n\nA data.frame: 10 × 8\n\n    MinMaxGoodBadGoodPBadPWOEIV\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    18231691100.052862060.088070456-0.510451290.0179721723\n    23285052310.157960590.184947958-0.157728920.0042566888\n    28335642130.176415390.170536429 0.033892450.0001992523\n    33384912120.153581480.169735789-0.100011790.0016156210\n    38434431830.138567410.146517214-0.055786020.0004434881\n    43483201230.100093840.098478783 0.016267000.0000262721\n    4853317 790.099155460.063250600 0.449584280.0161422597\n    5358214 580.066937750.046437150 0.365663370.0074963200\n    5863124 310.038786360.024819856 0.446424720.0062349937\n    6368 50  90.015639660.007205765 0.774928790.0065356700\n\n\n\n\nСумарний IV:\n\nage_iv <- sum(age_groups$IV)\nage_iv\n\n0.0609227380464588\n\n\nВізуалізуємо значення груп WOE:\n\nbarplot(age_groups$WOE, \n        col=\"brown\", \n        names.arg=c(age_groups$Min), \n        xlab=\"Min Age\",\n        ylab=\"WOE\"\n)\n\n\n\n\nЗамінимо значення на WOE:\n\nnew_df$Age <- data$Age\nnew_df$AgeWoe <- c(0)\n\nfor(i in 1:nrow(age_groups)) {\n  \n  min <- age_groups$Min[i]\n  max <- age_groups$Max[i]\n  woe <- age_groups$WOE[i]\n  \n  new_df[new_df$Age >= min & new_df$Age < max, ]$AgeWoe <- woe\n  \n  if(i == nrow(age_groups)) {\n    new_df$AgeWoe[i] <- woe\n  }\n  \n}\n\nhead(new_df)\n\n\n\nA data.frame: 6 × 5\n\n    StatusHomeHomeWoeAgeAgeWoe\n    <chr><chr><dbl><dbl><dbl>\n\n\n    1goodrent -0.529263130 0.03389245\n    2goodrent -0.529263158 0.44642472\n    3bad owner 0.541734946 0.01626700\n    4goodrent -0.529263124-0.15772892\n    5goodrent -0.529263126-0.15772892\n    6goodowner 0.541734936-0.10001179\n\n\n\n\nВидалимо оригінальні значення з набору даних:\n\nnew_df$Home <- NULL\nnew_df$Age <- NULL\nhead(new_df)\n\n\n\nA data.frame: 6 × 3\n\n    StatusHomeWoeAgeWoe\n    <chr><dbl><dbl>\n\n\n    1good-0.5292631 0.03389245\n    2good-0.5292631 0.44642472\n    3bad  0.5417349 0.01626700\n    4good-0.5292631-0.15772892\n    5good-0.5292631-0.15772892\n    6good 0.5417349-0.10001179"
  },
  {
    "objectID": "030-supervised-learning.html",
    "href": "030-supervised-learning.html",
    "title": "ТЕМА 3. НАВЧАННЯ “З УЧИТЕЛЕМ”",
    "section": "",
    "text": "У даній темі розкрито основні принципи побудови та використання регресійних моделей, а також моделей для розв’язання задачк ласифікації.\nПід час вивчення матеріалів розглядаються алгоритми на основі лінійної та логістичної регресії, нейронних мереж, дерев рішень та градієнтного бустінгу.\nДля практичного закріплення знань використовують можливості бібліотек R: caret, neuralnet, gmodels, rpart та інші.\n\n\nЦе вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html",
    "href": "031-supervised-learning-linear-regression-1.html",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "",
    "text": "Курс: Математичне моделювання в R"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#dataset-overview",
    "href": "031-supervised-learning-linear-regression-1.html#dataset-overview",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.1 Dataset overview",
    "text": "2.1 Dataset overview\nУ даному навчальному матеріалі використано класичний приклад даних з інформацією про престижність професій у Канаді 1971 року. Джерело: carData::Prestige.\n\nlibrary(carData)\ndata <- carData::Prestige\nhead(data)\n\n\n\nA data.frame: 6 × 6\n\n    educationincomewomenprestigecensustype\n    <dbl><int><dbl><dbl><int><fct>\n\n\n    gov.administrators13.111235111.1668.81113prof\n    general.managers12.2625879 4.0269.11130prof\n    accountants12.77 927115.7063.41171prof\n    purchasing.officers11.42 8865 9.1156.81175prof\n    chemists14.62 840311.6873.52111prof\n    physicists15.6411030 5.1377.62113prof\n\n\n\n\nПереглянемо структуру даних:\n\nstr(data)\n\n'data.frame':   102 obs. of  6 variables:\n $ education: num  13.1 12.3 12.8 11.4 14.6 ...\n $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...\n $ women    : num  11.16 4.02 15.7 9.11 11.68 ...\n $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...\n $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...\n $ type     : Factor w/ 3 levels \"bc\",\"prof\",\"wc\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nЗначення показників вибірки:\n\nprestige – престиж професії за Pineo-Porter score, на основі дослідження середини 1960-х. It is target!!!\neducation - середня кількість років освіти.\nincome – середній дохід респондентів, дол.\nwomen – частка жінок у галузі\ncensus – канадський код професії.\ntype – тип професії: bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar.\n\nОглянемо описову статистику факторів:\n\nsummary(data)\n\n   education          income          women           prestige    \n Min.   : 6.380   Min.   :  611   Min.   : 0.000   Min.   :14.80  \n 1st Qu.: 8.445   1st Qu.: 4106   1st Qu.: 3.592   1st Qu.:35.23  \n Median :10.540   Median : 5930   Median :13.600   Median :43.60  \n Mean   :10.738   Mean   : 6798   Mean   :28.979   Mean   :46.83  \n 3rd Qu.:12.648   3rd Qu.: 8187   3rd Qu.:52.203   3rd Qu.:59.27  \n Max.   :15.970   Max.   :25879   Max.   :97.510   Max.   :87.20  \n     census       type   \n Min.   :1113   bc  :44  \n 1st Qu.:3120   prof:31  \n Median :5135   wc  :23  \n Mean   :5402   NA's: 4  \n 3rd Qu.:8312            \n Max.   :9517"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#traintest-split",
    "href": "031-supervised-learning-linear-regression-1.html#traintest-split",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.2 Train/Test split",
    "text": "2.2 Train/Test split\nРозділимо загальну вибірку на 2 частини: * тренувальна, 70% вибірки, для побудови регресії; * тестова, 30% вибірки, для перевірки точності моделі.\n\nset.seed(10) #довільне число як точка \"відправки\" для генератора випадкових чисел\n\n# Згенеруємо набір чисел від 1 до кількості спостережень у вибірці і відберемо випадквоим чином 70% із них\ntrain_index <- sample(nrow(data), size = 0.7*nrow(data))\n\n#Виведемо індекси (номери рядків) обраних для тренування даних\nprint(train_index)\n\n [1]   9  74  76  55  72  54  39  83  88  15  93  42  71 101  34  24  13   8   7\n[20]  27  82  29  81  50  26  33  84  78  79  30  68  51  97  59  32  11  77  91\n[39]  28  95  65  14  86  66  41  25  85  16  53  75  57  17  48  23  92  46  87\n[58]  94   4  35  61  69  43  10  96  99  89  31  38  52  18\n\n\n\n#Запишемо по номерах відібраних рядків тренувальний набір даних\ntrain_data <- data[train_index, ]\nhead(train_data)\n\n#Всі інші значення, що не увійшли в тренувальну вибірку запишемо у тестову\ntest_data <- data[-train_index, ]\nhead(test_data)\n\n\n\nA data.frame: 6 × 6\n\n    educationincomewomenprestigecensustype\n    <dbl><int><dbl><dbl><int><fct>\n\n\n    civil.engineers14.5211377 1.0373.12143prof\n    textile.weavers 6.69 444331.3633.38267bc  \n    tool.die.makers10.09 8043 1.5042.58311bc  \n    insurance.agents11.60 813113.0947.35171wc  \n    slaughterers.2 7.64 513417.2634.88215bc  \n    service.station.attendant 9.93 2370 3.6923.35145bc  \n\n\n\n\n\n\nA data.frame: 6 × 6\n\n    educationincomewomenprestigecensustype\n    <dbl><int><dbl><dbl><int><fct>\n\n\n    gov.administrators13.111235111.1668.81113prof\n    general.managers12.2625879 4.0269.11130prof\n    accountants12.77 927115.7063.41171prof\n    chemists14.62 840311.6873.52111prof\n    physicists15.6411030 5.1377.62113prof\n    draughtsmen12.30 7059 7.8360.02163prof"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#short-eda-exploratory-data-analysis",
    "href": "031-supervised-learning-linear-regression-1.html#short-eda-exploratory-data-analysis",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.3 Short EDA (Exploratory data analysis)",
    "text": "2.3 Short EDA (Exploratory data analysis)\n\n2.3.1 Correlation\nПереглянемо наявність зв’язків між параметрами за допоомгою матриці попарних кореляцій. Дані на перетині рядків вказують на рівень кореляції між вибраними показниками.\nВиключимо змінну type, оскільки вона не має числового представлення:\n\ncor(train_data[ ,-6])\n\n\n\nA matrix: 5 × 5 of type dbl\n\n    educationincomewomenprestigecensus\n\n\n    education 1.0000000 0.6243784 0.1507510 0.8448752-0.8212488\n    income 0.6243784 1.0000000-0.3910854 0.7425534-0.3037354\n    women 0.1507510-0.3910854 1.0000000 0.0227754-0.3431387\n    prestige 0.8448752 0.7425534 0.0227754 1.0000000-0.5968525\n    census-0.8212488-0.3037354-0.3431387-0.5968525 1.0000000\n\n\n\n\nLets view correlation matrix with corrplot() function:\n\nsuppressMessages(library(corrplot))\ncorrplot(cor(train_data[,-6]) , method = \"number\") \n\n\n\n\nYou can see hight correlation between education ~ prestige, education ~ census, prestige ~ income.\n\n\n2.3.2 Visual analysis\nLets check data distribution of some variables\nTarget/Output variable prestige:\n\nlibrary(ggplot2)\n\nggplot(train_data, aes(prestige)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nInput variable income:\n\n#ggplot(train_data, aes(log(income))) + \nggplot(train_data, aes(income)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nInput variable women:\n\n#ggplot(train_data, aes(log(women))) + \nggplot(train_data, aes(women)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#model-building",
    "href": "031-supervised-learning-linear-regression-1.html#model-building",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.4 Model building",
    "text": "2.4 Model building\nФункція lm() використовується для побудови лінійної регресії.\nSyntax: lm(formula, data = train_data)\nformula дозволяє вказати на залежність між вхідними та вихідним параметром. У даному випадку prestige - залежна змінна (Y), а усі, що після знаку ~ - незалежні (X).\n\nlm_mod <- lm(formula = prestige ~ income + education, data = train_data)\n\nДля детальнішого опису параметрів побудованої моделі варто скористатися функцією summary():\n\nsummary(lm_mod)\n\n\nCall:\nlm(formula = prestige ~ income + education, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.2951  -4.7988   0.1218   5.1932  17.1181 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -5.9237106  3.9637472  -1.494     0.14    \nincome       0.0014932  0.0003017   4.949 5.18e-06 ***\neducation    3.9135175  0.4460247   8.774 8.64e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.035 on 68 degrees of freedom\nMultiple R-squared:  0.7896,    Adjusted R-squared:  0.7834 \nF-statistic: 127.6 on 2 and 68 DF,  p-value: < 2.2e-16\n\n\n\n\n2.4.1 Demo task: generating final formula for regression\nСпробуємо створити рядковий вигляд для побудованої регресії (шляхом звичайного “склеювання” рядків). Це завдання для закріплення знань з алгоритмічного програмування в R.\n\nlm_mod$coefficients # coefficients of the model\n\n(Intercept)-5.92371064968481income0.00149322824686838education3.91351754603805\n\n\n\ngetLmFormula <- function(lm_model) {\n  \n  str_formula <- paste(lm_model$terms[[2]], \" = \", sep=\"\")\n  str_formula <- paste(str_formula, + round(lm_model$coefficients[1], 4), sep=\" \")\n  \n  for(i in 2:length(lm_mod$coefficients)) {\n    \n    znak <- \"+\"\n    if(lm_model$coefficients[i] < 0)\n      znak <- \"-\"\n    str_formula <- paste(str_formula, \" \", znak, \" \", round(lm_mod$coefficients[i], 4), \"*\", names(lm_mod$coefficients)[i] ,sep = \"\")\n  }\n  \n  return(str_formula)\n}\n\nТоді формулу можна отримати наступним чином:\n\nstr_formula <- getLmFormula(lm_mod)\nprint(str_formula)\n\n[1] \"prestige =  -5.9237 + 0.0015*income + 3.9135*education\"\n\n\nEND OF DEMO TASK\n\nOne more way to preview model info is using package broom and function tidy():\n\n#install.packages(\"broom\")\nlibrary(broom)\n\nlm_mod_view <- tidy(lm_mod)\nlm_mod_view\n\n\n\nA tibble: 3 × 5\n\n    termestimatestd.errorstatisticp.value\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    (Intercept)-5.9237106503.9637471616-1.4944721.396785e-01\n    income      0.0014932280.0003017277 4.9489275.183124e-06\n    education   3.9135175460.4460246832 8.7742178.636723e-13\n\n\n\n\n\n\n2.4.2 Зміна форми залежності\nСпробуємо змінити форму залежності і побудуємо модель на основі трансформованих показників. Скористаємося логарифмуванням незалежних змінних:\n\nlm_mod2 <- lm(formula = prestige ~ log(income) + log(education), data = train_data)\nsummary(lm_mod2)\n\n\nCall:\nlm(formula = prestige ~ log(income) + log(education), data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.6827  -6.0052   0.3465   4.1241  17.4622 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -165.835     14.142 -11.727  < 2e-16 ***\nlog(income)      14.016      1.923   7.290 4.25e-10 ***\nlog(education)   38.615      4.094   9.433 5.62e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.496 on 68 degrees of freedom\nMultiple R-squared:  0.8169,    Adjusted R-squared:  0.8115 \nF-statistic: 151.7 on 2 and 68 DF,  p-value: < 2.2e-16\n\n\n\\(R^2\\) зріс. Тобто зміна форми залежності може впливати на якість моделі.\nAlert! Its depends on our seed parameter, because of changing train and test sets.\nПовернемося до попередньої моделі:\n\nlm_mod <- lm(formula = prestige ~ income + education, data = train_data)\n\nПереглянемо графік реальних даних, прогнозованих та похибок. Для початку створимо тимчасовий data.frame для генерації графіка зі значень\n\nфактори;\nреальні значення;\nпрогнозовані значення;\nпохибки.\n\nУвага! Даний графік будуватимемо на тренувальній вибірці!\n\n# data frame for storing data\ntmp_data <- data.frame(education = train_data$education,\n                       income = train_data$income,\n                       prestige = train_data$prestige,\n                       predicted = lm_mod$fitted.values,\n                       residuals = lm_mod$residuals) \n\nПереглянемо залежності між параметрами моделі:\n\n# prestige vs education\nggplot(tmp_data, aes(x = education, y = prestige)) +\n  geom_segment(aes(xend = education, yend = predicted), alpha = .2) +\n  geom_point(aes(), color = 'blue') +\n  #scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  #guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 2) +\n  theme_bw()\n\n\n\n\n\n# prestige vs income\nggplot(tmp_data, aes(x = income, y = prestige)) +\n  geom_segment(aes(xend = income, yend = predicted), alpha = .2) +\n  geom_point(aes(color = residuals)) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 2) +\n  theme_bw()"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#prediction-on-test-data-and-accuracy",
    "href": "031-supervised-learning-linear-regression-1.html#prediction-on-test-data-and-accuracy",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.5 Prediction on test data and accuracy",
    "text": "2.5 Prediction on test data and accuracy\nЗдійснимо прогноз за допомогою функції predict().\n\nsuppressMessages(library(dplyr))\n\ntest_predicted <- predict(lm_mod, test_data)\n\nsummary(test_predicted)\nas.data.frame(sort(test_predicted) %>% head())\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.94   33.43   43.74   45.96   56.82   80.70 \n\n\n\n\nA data.frame: 6 × 1\n\n    sort(test_predicted) %>% head()\n    <dbl>\n\n\n    canners25.93679\n    farmers26.28458\n    launderers27.24206\n    cooks29.01981\n    bakers29.85428\n    babysitters32.01053\n\n\n\n\nMSE (Mean Squared Error) – середньоквадратичне відхилення; середнє значення квадратів відхилень прогнозованих даних від реальних.\n\nmse <- mean((test_data$prestige - test_predicted)^2)\nmse\n\n55.2458474568378\n\n\nMAPE. MAPE (Mean Absolute Percentage Error) – середнє абсолютне відхилення прогнозованого показника від реального:\n\nmape <- mean(abs(test_data$prestige - test_predicted)/test_data$prestige)\nmape\n\n0.134424776204161\n\n\nОбчислимо метрики моделі за допомогою пакету modelr та функцій з нього:\n\nsuppressMessages(library(modelr))\ndata.frame(\n  R2 = rsquare(lm_mod, data = test_data),\n  MSE = mse(lm_mod, data = test_data),\n  RMSE = rmse(lm_mod, data = test_data),\n  MAE = mae(lm_mod, data = test_data),\n  MAPE = mape(lm_mod, data = test_data)\n)\n\n\n\nA data.frame: 1 × 5\n\n    R2MSERMSEMAEMAPE\n    <dbl><dbl><dbl><dbl><dbl>\n\n\n    0.821986555.245857.4327556.1394070.1344248\n\n\n\n\nMae - Mean Absolute Error. Rmse - Root mean squared error. Детальніше: https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d.\n\n2.5.1 Перевірка на мультиколінераність\nЗафіксуємо значення R^2 для моделі у деякій змінній:\n\nr_sq = rsquare(lm_mod, data = test_data)\nr_sq \n\n0.821986517080715\n\n\n\nsuppressMessages(library(car))\n\n\nvif(lm_mod)\n\nincome1.63893675540857education1.63893675540857\n\n\nЯк видно вище тут мультиколінеарність відсутня.\nВведемо в модель додатковий показник, що буде явно залежати від одного з факторів:\n\n#lm_mod2 <- lm(formula = prestige ~ income + education + income_2, data = train_data)\n#summary(lm_mod2)\n#rsquare(lm_mod2, data = train_data)\n\n\n#r_sq2 <- rsquare(lm_mod2, data = test_data)\n#r_sq\n#r_sq2\n\nVIF-тест для моделі із корельованими змінними:\n\n#vif(lm_mod2)\n\nThe best way is to exclude the biggest and rebuild model in this case.\n\n\n2.5.2 Extending model\nДодамо до моделі категоріальний параметр type. Переглянемо можливі варіанти значень:\n\nunique(train_data$type)\n\n\nprofbcwc<NA>\n\n\n    \n        Levels:\n    \n    \n    'bc''prof''wc'\n\n\n\nSo, its a factor. with 3 values and missing data (NA).\nLets build new model with type:\n\nlm_mod <- lm(formula = prestige ~ income + education + type, data = train_data)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = prestige ~ income + education + type, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.7801  -4.1603   0.0212   5.2034  20.3020 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.136361   6.409402   0.645 0.521002    \nincome       0.001185   0.000303   3.912 0.000224 ***\neducation    2.962932   0.813449   3.642 0.000542 ***\ntypeprof     8.953012   4.713668   1.899 0.062024 .  \ntypewc      -0.974902   3.250061  -0.300 0.765177    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.434 on 64 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8214,    Adjusted R-squared:  0.8102 \nF-statistic: 73.59 on 4 and 64 DF,  p-value: < 2.2e-16\n\n\nЗ’явилися нові показники typeprof та typewc. Вони згенеровані автоматично як dummy-змінні функцією lm(). Якщо Ви хочете переглянути усі показники, що приймали участь у побудові моделі, то можна викликати $model:\n\nlm_mod$model %>% head()\n\n\n\nA data.frame: 6 × 4\n\n    prestigeincomeeducationtype\n    <dbl><int><dbl><fct>\n\n\n    civil.engineers73.11137714.52prof\n    textile.weavers33.3 4443 6.69bc  \n    tool.die.makers42.5 804310.09bc  \n    insurance.agents47.3 813111.60wc  \n    slaughterers.234.8 5134 7.64bc  \n    service.station.attendant23.3 2370 9.93bc  \n\n\n\n\n\nlm_mod$model\n\n\n\nA data.frame: 69 × 4\n\n    prestigeincomeeducationtype\n    <dbl><int><dbl><fct>\n\n\n    civil.engineers73.11137714.52prof\n    textile.weavers33.3 4443 6.69bc  \n    tool.die.makers42.5 804310.09bc  \n    insurance.agents47.3 813111.60wc  \n    slaughterers.234.8 5134 7.64bc  \n    service.station.attendant23.3 2370 9.93bc  \n    computer.operators47.7 433011.36wc  \n    radio.tv.repairmen37.2 544910.29bc  \n    electrical.linemen40.9 8316 9.05bc  \n    psychologists74.9 740514.36prof\n    house.painters29.9 4549 7.81bc  \n    receptionsts38.7 290111.04wc  \n    slaughterers.125.2 5134 7.64bc  \n    typesetters42.2 646210.00bc  \n    physicians87.22530815.96prof\n    computer.programers53.8 842513.83prof\n    architects78.11416315.44prof\n    biologists72.6 825815.09prof\n    nurses64.7 461412.46prof\n    electronic.workers50.8 3942 8.76bc  \n    physio.therapsts72.1 509213.62prof\n    aircraft.workers43.7 6573 8.78bc  \n    sales.supervisors41.5 7482 9.84wc  \n    osteopaths.chiropractors68.41749814.71prof\n    radio.tv.announcers57.6 756212.71wc  \n    sewing.mach.operators28.2 2847 6.38bc  \n    sheet.metal.workers35.9 6565 8.40bc  \n    welders41.8 6477 7.92bc  \n    pharmacists69.31043215.21prof\n    farm.workers21.5 1656 8.60bc  \n    ⋮⋮⋮⋮⋮\n    janitors17.3 3472 7.11bc  \n    economists62.2 804914.44prof\n    aircraft.repairmen50.3 771610.10bc  \n    elevator.operators20.1 3582 7.58bc  \n    file.clerks32.7 301612.09wc  \n    veterinarians66.71455815.94prof\n    auto.repairmen38.1 5795 8.10bc  \n    social.workers55.1 633614.21prof\n    textile.labourers28.8 3485 6.74bc  \n    buyers51.1 795611.03wc  \n    lawyers82.31926315.77prof\n    travel.clerks35.7 625911.43wc  \n    secondary.school.teachers66.1 803415.08prof\n    masons36.2 5959 6.60bc  \n    collectors29.4 474111.20wc  \n    railway.sectionmen27.3 4696 6.67bc  \n    plumbers42.9 6928 8.33bc  \n    purchasing.officers56.8 886511.42prof\n    secretaries46.0 403611.59wc  \n    bartenders20.2 3930 8.50bc  \n    rotary.well.drillers35.3 6860 8.88bc  \n    mail.carriers36.1 5511 9.22wc  \n    mining.engineers68.81102314.64prof\n    pilots66.11403212.27prof\n    taxi.drivers25.1 4224 7.93bc  \n    electricians50.2 7147 9.93bc  \n    medical.technicians67.5 518012.79wc  \n    tellers.cashiers42.3 244810.64wc  \n    sales.clerks26.5 259410.05wc  \n    librarians58.1 611214.15prof"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#model-errors",
    "href": "031-supervised-learning-linear-regression-1.html#model-errors",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.6 Model errors",
    "text": "2.6 Model errors\nПереглянемо характеристики моделі:\n\nlm_mod <- lm(formula = prestige ~ income + education + type, data = train_data)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = prestige ~ income + education + type, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.7801  -4.1603   0.0212   5.2034  20.3020 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.136361   6.409402   0.645 0.521002    \nincome       0.001185   0.000303   3.912 0.000224 ***\neducation    2.962932   0.813449   3.642 0.000542 ***\ntypeprof     8.953012   4.713668   1.899 0.062024 .  \ntypewc      -0.974902   3.250061  -0.300 0.765177    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.434 on 64 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8214,    Adjusted R-squared:  0.8102 \nF-statistic: 73.59 on 4 and 64 DF,  p-value: < 2.2e-16\n\n\nСформуємо розмітку для виведення графіків 4-х графіків одразу (2*2):\n\n#par(mfrow=c(2,2))\nplot(lm_mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\nНа двох графіках зліва червона лінія показує середнє з відхилень. Якщо варіація похибок зростає разом із збільшенням значень прогнозу - це називається гетероскедастичніть. Прогнозування за таких умов буде давати спотворені результати. Переконаємося у відсутності гетероскедастичності перевіривши p-value:\n\nsuppressMessages(library(lmtest))\nbptest(lm_mod)\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm_mod\nBP = 9.0675, df = 4, p-value = 0.05943\n\n\np-value > 0.05, отже ми відхиляємо гіпотезу про те, що гетероскедастичність відсутня (залишки гомоскедастичні - мають однакову дисперсію).\nNormal QQ-графік дозволяє перевірити чи похибки розподілені за нормальним законом розподілу. Ідеальний варіант коли вони розміщені чітко по діагональній лінії.\nНа останньому графіку показується як кожне значення впливає на регресію. У статистиці відстань Кука є загальноприйнятою оцінкою впливу спостереження під час застосування методу найменших квадратів у регресійному аналізі.На практиці, при застосуванні методу найменших квадратів, відстань Кука може використовуватися для наступних цілей: визначити впливові спостереження даних, які потрібно перевірити на валідність; визначення областей простору, у яких непогано було б отримати більше результатів спостереження. Джерело: Detection of Influential Observation in Linear Regression / R. Dennis Cook (https://www.jstor.org/stable/1268249?origin=crossref&seq=1)\nПовернемо розмітку для 1-го графіка у вікні RStudio:\n\npar(mfrow=c(1,1))\n\nПереглянемо розподіл похибок:\n\nplot(residuals(lm_mod)) \nabline(a=0,b=0,col='blue')\n\n\n\n\nВізуальних закономірностей у похибках немає. Схоже, що вони мають випадковий характер."
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#features-selection",
    "href": "031-supervised-learning-linear-regression-1.html#features-selection",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.7 Features selection",
    "text": "2.7 Features selection\nРучний відбір параметрів моделі має ряд недоліків, що пов’язані із якістю моделі та затратами часу на її побудову. Вивчимо окремі алгоритми автоматизованого відбору параметрів у модель.\n\n2.7.1 BestSubsets method\nМетод BestSubsets також відомий як all possible regressions. Цей метод відповідно до назв будує всі можливі варіанти моделей на незалежних змінних. Кількість моделей становить \\(2^p\\), де \\(p\\) - кількість неазелжних змінних, так для 10 вхідних параметрів буде побудовано \\(2^10 =1024\\) моделей.\n\nlibrary(leaps)\nbest_subsets_mod <- regsubsets(prestige ~ income + education + type + women, data = train_data)\n\nРозгялнемо відібрані параметри моделі BestSubsets. По осі Y розміщені значення BIC, який обраний як якісний показник регресії, по X - параметри моделі:\n\nplot(best_subsets_mod)\n\n\n\n\nНайтемніша лінія вказує на найкращу модель.\nПеребудуємо модель для усіх можливих варіантів комбінацій параметрів:\n\nbest_subsets_mod <- regsubsets(prestige ~ income + education + type + women, data = train_data)\n\nТакож можемо обрати кращу модель окремо за критеріями, наприклад скоригований \\(R^2\\):\n\nplot(best_subsets_mod, scale = \"adjr2\")\n\n\n\n\nМоделі з найвищим \\(R^2\\) обираються у такому випадку за цим критерієм, проте не варто забувати про появу мультиколінеарності.\n\nresults <- summary(best_subsets_mod)\nround(results$adjr2, 4)\n\n\n0.7290.79030.81290.81510.8136\n\n\n\nmodel_index <- which.max(results$adjr2)\nt <- results$which[model_index, ]\nas.data.frame(t)\n\n\n\nA data.frame: 6 × 1\n\n    t\n    <lgl>\n\n\n    (Intercept) TRUE\n    income TRUE\n    education TRUE\n    typeprof TRUE\n    typewcFALSE\n    women TRUE\n\n\n\n\nSo, the best regression by Adjusted R-Squared is\na0 + a1*income + a2*typeprof + a3*income:typeprof + a4*education:women + a5*income:education:typeprof + a6*income:typeprof:women + a7*income:typewc:women + a8*income:education:typewc:women\n\n\n\n2.7.2 Stepwise method\nStepwise - метод, що перебирає можливі варіанти та повертає найкращу модель з найнижчим показником AIC. Перебір моделі може бути з виключенням або з включенням показників у модель - у обох напрямках.\nДля початку очистимо дані, що мають пропуски у полі type (також ці дані можна заповнити).\n\nnrow(train_data)\ntrain_data <- train_data[!is.na(train_data$type), ]\nnrow(train_data)\n\n71\n\n\n69\n\n\nНайпростіша модель з одним параметром (першим по списку) матиме вигляд:\n\nstart_mod <- lm(prestige ~ 1, data = train_data)\n\nМодель з усіма параметрами:\n\nend_mod <- lm(prestige ~ ., data = train_data)\n\nЗапустимо алгоритм функцією step():\n\nstepwise_mod <- step(start_mod, \n                     # set minimum and maximum parameters\n                     scope = list(lower = start_mod, upper = end_mod),\n                     # direction of model building\n                     direction = \"both\", trace = 1, steps = 1000)\n                     # c(\"both\", \"backward\", \"forward\") - possible directions\n\nStart:  AIC=392.5\nprestige ~ 1\n\n            Df Sum of Sq     RSS    AIC\n+ education  1   14514.4  5286.9 303.38\n+ type       2   13466.4  6334.9 317.86\n+ income     1   10623.3  9178.0 341.44\n+ census     1    7498.0 12303.3 361.66\n<none>                   19801.3 392.50\n+ women      1       2.1 19799.2 394.49\n\nStep:  AIC=303.38\nprestige ~ education\n\n            Df Sum of Sq     RSS    AIC\n+ income     1    1257.5  4029.4 286.64\n+ type       2     904.7  4382.2 294.43\n+ census     1     512.7  4774.3 298.35\n+ women      1     284.3  5002.6 301.57\n<none>                    5286.9 303.38\n- education  1   14514.4 19801.3 392.50\n\nStep:  AIC=286.64\nprestige ~ education + income\n\n            Df Sum of Sq    RSS    AIC\n+ type       2     492.9 3536.5 281.64\n<none>                   4029.4 286.64\n+ women      1      70.7 3958.8 287.42\n+ census     1      47.1 3982.3 287.83\n- income     1    1257.5 5286.9 303.38\n- education  1    5148.6 9178.0 341.44\n\nStep:  AIC=281.64\nprestige ~ education + income + type\n\n            Df Sum of Sq    RSS    AIC\n+ census     1    144.45 3392.1 280.76\n+ women      1    116.49 3420.0 281.33\n<none>                   3536.5 281.64\n- type       2    492.89 4029.4 286.64\n- education  1    733.13 4269.7 292.64\n- income     1    845.72 4382.2 294.43\n\nStep:  AIC=280.76\nprestige ~ education + income + type + census\n\n            Df Sum of Sq    RSS    AIC\n+ women      1    131.49 3260.6 280.03\n<none>                   3392.1 280.76\n- census     1    144.45 3536.5 281.64\n- income     1    470.45 3862.5 287.72\n- type       2    590.20 3982.3 287.83\n- education  1    877.55 4269.6 294.64\n\nStep:  AIC=280.03\nprestige ~ education + income + type + census + women\n\n            Df Sum of Sq    RSS    AIC\n<none>                   3260.6 280.03\n- women      1    131.49 3392.1 280.76\n- census     1    159.46 3420.0 281.33\n- type       2    628.53 3889.1 288.20\n- income     1    596.05 3856.6 289.62\n- education  1    748.59 4009.2 292.29\n\n\n\n# lets see the summary\nsummary(stepwise_mod)\n\n\nCall:\nlm(formula = prestige ~ education + income + type + census + \n    women, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.0817  -4.9121   0.0985   5.8624  19.7564 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.136e+01  1.058e+01  -1.073 0.287294    \neducation    3.328e+00  8.821e-01   3.773 0.000363 ***\nincome       1.307e-03  3.882e-04   3.367 0.001311 ** \ntypeprof     1.241e+01  5.318e+00   2.333 0.022930 *  \ntypewc       7.615e-01  3.732e+00   0.204 0.839003    \ncensus       1.356e-03  7.788e-04   1.741 0.086595 .  \nwomen        6.212e-02  3.928e-02   1.581 0.118911    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.252 on 62 degrees of freedom\nMultiple R-squared:  0.8353,    Adjusted R-squared:  0.8194 \nF-statistic: 52.42 on 6 and 62 DF,  p-value: < 2.2e-16\n\n\n\nЗ підсумків моделі видно, що детермінація зросла, тобто модель стала описувати явище ще краще."
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html",
    "href": "032-supervised-learning-linear-regression-2.html",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "",
    "text": "Курс: “Математичне моделювання в R”"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#dataset-overview",
    "href": "032-supervised-learning-linear-regression-2.html#dataset-overview",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.1 Dataset overview",
    "text": "3.1 Dataset overview\nSource: https://www.kaggle.com/c/bike-sharing-demand/data\nDataset description:\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\nData Fields:\n\ndatetime - hourly date + timestamp\nseason -\n1 = spring\n2 = summer\n3 = fall\n4 = winter\n\nholiday - whether the day is considered a holiday\nworkingday - whether the day is neither a weekend nor holiday\nweather\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\ntemp - temperature in Celsius\natemp - “feels like” temperature in Celsius\nhumidity - relative humidity\nwindspeed - wind speed\ncasual - number of non-registered user rentals initiated\nregistered - number of registered user rentals initiated\ncount - number of total rentals (It is target variable. We will predict it!)\n\n\ndata <- read.csv(\"data/bikes.csv\")\nhead(data)\n\n\n\nA data.frame: 6 × 12\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcasualregisteredcount\n    <chr><int><int><int><int><dbl><dbl><int><dbl><int><int><int>\n\n\n    12011-01-01 00:00:0010019.8414.395810.000031316\n    22011-01-01 01:00:0010019.0213.635800.000083240\n    32011-01-01 02:00:0010019.0213.635800.000052732\n    42011-01-01 03:00:0010019.8414.395750.000031013\n    52011-01-01 04:00:0010019.8414.395750.00000 1 1\n    62011-01-01 05:00:0010029.8412.880756.00320 1 1\n\n\n\n\nОписова статистика факторів:\n\nsummary(data)\n\n   datetime             season         holiday          workingday    \n Length:10886       Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :3.000   Median :0.00000   Median :1.0000  \n                    Mean   :2.507   Mean   :0.02857   Mean   :0.6809  \n                    3rd Qu.:4.000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :4.000   Max.   :1.00000   Max.   :1.0000  \n    weather           temp           atemp          humidity     \n Min.   :1.000   Min.   : 0.82   Min.   : 0.76   Min.   :  0.00  \n 1st Qu.:1.000   1st Qu.:13.94   1st Qu.:16.66   1st Qu.: 47.00  \n Median :1.000   Median :20.50   Median :24.24   Median : 62.00  \n Mean   :1.418   Mean   :20.23   Mean   :23.66   Mean   : 61.89  \n 3rd Qu.:2.000   3rd Qu.:26.24   3rd Qu.:31.06   3rd Qu.: 77.00  \n Max.   :4.000   Max.   :41.00   Max.   :45.45   Max.   :100.00  \n   windspeed          casual         registered        count      \n Min.   : 0.000   Min.   :  0.00   Min.   :  0.0   Min.   :  1.0  \n 1st Qu.: 7.002   1st Qu.:  4.00   1st Qu.: 36.0   1st Qu.: 42.0  \n Median :12.998   Median : 17.00   Median :118.0   Median :145.0  \n Mean   :12.799   Mean   : 36.02   Mean   :155.6   Mean   :191.6  \n 3rd Qu.:16.998   3rd Qu.: 49.00   3rd Qu.:222.0   3rd Qu.:284.0  \n Max.   :56.997   Max.   :367.00   Max.   :886.0   Max.   :977.0"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#minimal-data-preprocessing",
    "href": "032-supervised-learning-linear-regression-2.html#minimal-data-preprocessing",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.2 Minimal data preprocessing",
    "text": "3.2 Minimal data preprocessing\nПеревіримо вибірку на наявність пропусків за допомогою функції md.pattern() з пакету mice:\n\nanyNA(data)\n\nFALSE\n\n\n\nsuppressMessages(library(mice))\nmd.pattern(data, F)\n\n /\\     /\\\n{  `---'  }\n{  O   O  }\n==>  V <==  No need for mice. This data set is completely observed.\n \\  \\|/  /\n  `-----'\n\n\n\n\n\nA matrix: 2 × 13 of type dbl\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcasualregisteredcount\n\n\n    108861111111111110\n    0000000000000\n\n\n\n\nThere are no missing data! Great for us!\nПеретворимо категоріальні змінні до факторів:\n\ndata$season <- factor(data$season)\ndata$holiday <- factor(data$holiday)\ndata$workingday <- factor(data$workingday)\ndata$weather <- factor(data$weather)\ndata$holiday <- factor(data$holiday)\n\nhead(data) # preview data\n\n\n\nA data.frame: 6 × 12\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcasualregisteredcount\n    <chr><fct><fct><fct><fct><dbl><dbl><int><dbl><int><int><int>\n\n\n    12011-01-01 00:00:0010019.8414.395810.000031316\n    22011-01-01 01:00:0010019.0213.635800.000083240\n    32011-01-01 02:00:0010019.0213.635800.000052732\n    42011-01-01 03:00:0010019.8414.395750.000031013\n    52011-01-01 04:00:0010019.8414.395750.00000 1 1\n    62011-01-01 05:00:0010029.8412.880756.00320 1 1\n\n\n\n\nУ наборі даних наявні 3 вихідних параметра: casual,registered,count, де $casual + registered = count$. Здійснимо прогнозcount`. Видалимо змінні, які використовуватися нами не будуть:\n\n#data$casual <- NULL\n#data$registered <- NULL\n\n# or with dplyr\nsuppressMessages(library(dplyr))\ndata <- data %>% select(-c(casual, registered))\n\nhead(data)\n\n\n\nA data.frame: 6 × 10\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcount\n    <chr><fct><fct><fct><fct><dbl><dbl><int><dbl><int>\n\n\n    12011-01-01 00:00:0010019.8414.395810.000016\n    22011-01-01 01:00:0010019.0213.635800.000040\n    32011-01-01 02:00:0010019.0213.635800.000032\n    42011-01-01 03:00:0010019.8414.395750.000013\n    52011-01-01 04:00:0010019.8414.395750.0000 1\n    62011-01-01 05:00:0010029.8412.880756.0032 1"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#exploratory-data-analysis",
    "href": "032-supervised-learning-linear-regression-2.html#exploratory-data-analysis",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.3 Exploratory data analysis",
    "text": "3.3 Exploratory data analysis\nОцінимо звязки між числовими змінними та вихідним показником через кореляцію:\n\ndata %>%\n select(temp:count) %>% \n cor() %>%\n as.data.frame()\n\n\n\nA data.frame: 5 × 5\n\n    tempatemphumiditywindspeedcount\n    <dbl><dbl><dbl><dbl><dbl>\n\n\n    temp 1.00000000 0.98494811-0.06494877-0.01785201 0.3944536\n    atemp 0.98494811 1.00000000-0.04353571-0.05747300 0.3897844\n    humidity-0.06494877-0.04353571 1.00000000-0.31860699-0.3173715\n    windspeed-0.01785201-0.05747300-0.31860699 1.00000000 0.1013695\n    count 0.39445364 0.38978444-0.31737148 0.10136947 1.0000000\n\n\n\n\nЩоб оглянути інформацію про частоту спостежень по сезонах варто скористатися записом:\n\ndata %>%\n group_by(season) %>%\n summarize(n = n())\n\n\n\nA tibble: 4 × 2\n\n    seasonn\n    <fct><int>\n\n\n    12686\n    22733\n    32733\n    42734\n\n\n\n\n\nlibrary(ggplot2)\n# llok like we have almost the same count by each season\nggplot(data, aes(season)) + \n    geom_bar(aes(fill = season)) + \n    theme_bw()\n\n\n\n\n\n# Holidays plot\ndata %>%\n group_by(holiday) %>%\n summarize(n = n())\n\nggplot(data, aes(holiday)) + geom_bar(aes(fill = holiday)) + theme_bw()\n\n\n\nA tibble: 2 × 2\n\n    holidayn\n    <fct><int>\n\n\n    010575\n    1  311\n\n\n\n\n\n\n\n\ndata %>%\n group_by(workingday) %>%\n summarize(n = n())\n\n# Working dats plot\nggplot(data, aes(workingday)) + geom_bar(aes(fill = workingday)) + theme_bw()\n\n\n\nA tibble: 2 × 2\n\n    workingdayn\n    <fct><int>\n\n\n    03474\n    17412\n\n\n\n\n\n\n\n\nweather\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\n\n\n# weather plot\nggplot(data, aes(weather)) + geom_bar(aes(fill = weather)) + theme_bw()\n\n\n\n\nПереглянемо також залежність між числовими факторами та кількістю прокатів байків. Для температури це матиме запис:\n\n# temp\nplot(x=data$temp, y=data$count)\n\n\n\n\n\nggplot(data, aes(temp, count)) + geom_point() + theme_bw()\n\n\n\n\n\n# atemp\nggplot(data, aes(atemp, count)) + geom_point() + theme_bw()\n\n\n\n\n\n# humidity\nggplot(data, aes(humidity, count)) + geom_point() + theme_bw()\n\n\n\n\n\n# windspeed\nggplot(data, aes(windspeed, count)) + geom_point() + theme_bw()\n\n\n\n\nПереглянемо також загальний розподіл частоти замовлень байків на гістограмі:\n\nggplot(data, aes(count)) + geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + theme_bw()\n\n\n\n\nПри спробі побудувати графік залежності між датою та кількістю замовлень байків ми отримаємо помилку або графік, що не відповідає порядку дат, оскільки на даний момент R не сприймає поле datetime як дату. Перетворимо дату за допомгою функції as.POSIXct():\n\ndata$datetime <- as.POSIXct(data$datetime)\nggplot(data, aes(datetime, count)) + geom_point() + theme_bw()\n\n\n\n\nЧи можна прослідкувати зміну кількості замовлень в залежності від пори року з цього графіку?\nПокращити вигляд графіка можна додавання параметрів у geom_point():\n\nggplot(data, aes(datetime, count)) + geom_point(aes(color=temp)) + theme_bw()\n\n\n\n\nПереглянемо як час оренди байка впливає на кількість замовлень:\n\nggplot(data, aes(format(datetime, \"%H\"), count)) + \n        geom_point(aes(color=temp), alpha = 0.5) + \n        theme_bw()\n\n\n\n\nAlso, we can add new column to our dataset and use it:\n\ndata <- data %>%\n    mutate(hour_rent = as.numeric(format(datetime, \"%H\")))\nhead(data)\n\n\n\nA data.frame: 6 × 11\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rent\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl>\n\n\n    12011-01-01 00:00:0010019.8414.395810.0000160\n    22011-01-01 01:00:0010019.0213.635800.0000401\n    32011-01-01 02:00:0010019.0213.635800.0000322\n    42011-01-01 03:00:0010019.8414.395750.0000133\n    52011-01-01 04:00:0010019.8414.395750.0000 14\n    62011-01-01 05:00:0010029.8412.880756.0032 15\n\n\n\n\n\n# the same chart as before\nggplot(data, aes(hour_rent, count)) + \n        geom_point(aes(color=temp), alpha = 0.5) + \n        theme_bw()"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#train-test-split",
    "href": "032-supervised-learning-linear-regression-2.html#train-test-split",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.4 Train test split",
    "text": "3.4 Train test split\nOne more splitting method is based on date. So, we can use first 70% of dataset as train and 30% as test. This method often used for timeseries forecasting. We will use it for linear regression for now.\n\nset.seed(10) \nnrow(data)\n\ntrain_count = ceiling(0.7 * nrow(data))\ntrain_count\n\ntest_count = nrow(data) - train_count\ntest_count\n\n10886\n\n\n7621\n\n\n3265\n\n\n\ndata <- data %>% arrange(datetime) # sort by datetime\n\n\ntrain_data <- data %>% slice_head(n = train_count)\nnrow(train_data)\ntest_data <- data %>% slice_tail(n = test_count)\nnrow(test_data)\n\n7621\n\n\n3265\n\n\n\n# lets compare last from train and first from test\ntail(train_data)\nhead(test_data) # ok!\n\n\n\nA data.frame: 6 × 11\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rent\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl>\n\n\n    76162012-05-16 16:00:00201129.5232.57545 7.001544616\n    76172012-05-16 17:00:00201129.5233.3355112.998087317\n    76182012-05-16 18:00:00201129.5233.3355115.001384618\n    76192012-05-16 19:00:00201128.7032.5755419.001259019\n    76202012-05-16 20:00:00201127.0631.0606516.997945920\n    76212012-05-16 21:00:00201126.2430.3057312.998039321\n\n\n\n\n\n\nA data.frame: 6 × 11\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rent\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl>\n\n\n    12012-05-16 22:00:00201126.2430.3057311.001428622\n    22012-05-16 23:00:00201125.4229.54578 7.001513323\n    32012-05-17 00:00:00201124.6028.79078 8.9981 79 0\n    42012-05-17 01:00:00201124.6028.0308311.0014 28 1\n    52012-05-17 02:00:00201124.6028.7907811.0014 16 2\n    62012-05-17 03:00:00201124.6029.5457316.9979  3 3"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#model-building",
    "href": "032-supervised-learning-linear-regression-2.html#model-building",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.5 Model building",
    "text": "3.5 Model building\nПобудуємо напройстішу модель залежності середньої температури за день та кількості орендованих байків:\n\nlm_bike <- lm(count ~ atemp, train_data)\nsummary(lm_bike)\n\n\nCall:\nlm(formula = count ~ atemp, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-254.42  -93.91  -27.43   65.57  648.09 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.0774     4.4973  -0.462    0.644    \natemp         7.2602     0.1895  38.311   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 138.4 on 7619 degrees of freedom\nMultiple R-squared:  0.1615,    Adjusted R-squared:  0.1614 \nF-statistic:  1468 on 1 and 7619 DF,  p-value: < 2.2e-16\n\n\nСпробуємо порахувати кількість орендованих байків для середніх темпертур від 10 до 30 градусів.\nСтворимо датафрейм з послідовністю елементів 10-30 з кроком 1 та порахуємо за формулою з інформації про модель прогнозовані показники:\nЗдійснимо прогноз за допомогою функції predict():\n\n#predict_1030$count_model <- predict(lm_bike, newdata = predict_1030)\n#predict_1030 %>% head()\n\n# our results are very close, minimal difference is a result of rounding numbers in our formula\n\n\n# how it looks like at chart\n#ggplot(predict_1030, aes(atemp, count_model)) + \n #       geom_point() + \n    #    theme_bw()\n\nПобудуємо модель для усіх параметрів моделі окрім дати:\n\nlm_bike <- lm(count~.-datetime, train_data)\nsummary(lm_bike)\n\n\nCall:\nlm(formula = count ~ . - datetime, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-283.67  -78.72  -25.68   46.27  587.17 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  15.67996    8.85332   1.771   0.0766 .  \nseason2       9.39674    4.59288   2.046   0.0408 *  \nseason3     -40.29356    6.46891  -6.229 4.95e-10 ***\nseason4      21.14562    4.40422   4.801 1.61e-06 ***\nholiday1     -8.46508    8.79408  -0.963   0.3358    \nworkingday1  -4.89327    3.12276  -1.567   0.1172    \nweather2      2.76675    3.47283   0.797   0.4257    \nweather3    -31.30849    5.61763  -5.573 2.59e-08 ***\nweather4     85.56066  122.62585   0.698   0.4854    \ntemp          1.34075    1.67470   0.801   0.4234    \natemp         6.30809    1.48446   4.249 2.17e-05 ***\nhumidity     -1.49856    0.08884 -16.868  < 2e-16 ***\nwindspeed     0.35086    0.19131   1.834   0.0667 .  \nhour_rent     6.20760    0.21640  28.685  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 7607 degrees of freedom\nMultiple R-squared:  0.3438,    Adjusted R-squared:  0.3427 \nF-statistic: 306.6 on 13 and 7607 DF,  p-value: < 2.2e-16\n\n\nПеревіримо модель на мультиколінеарність:\n\nsuppressMessages(library(car))\nvif(lm_bike)\n\n\n\nA matrix: 9 × 3 of type dbl\n\n    GVIFDfGVIF^(1/(2*Df))\n\n\n    season 2.94174431.197018\n    holiday 1.07616211.037382\n    workingday 1.07216311.035453\n    weather 1.29120731.043516\n    temp83.07373919.114480\n    atemp78.28136218.847676\n    humidity 1.59412411.262586\n    windspeed 1.33172311.154003\n    hour_rent 1.13543711.065569\n\n\n\n\nВиключимо корельовані показники з моделі за принципом - залишити той, що має вищу кореляцію із залежним фактором. У нашому випадку це temp:\n\nlm_bike <- lm(count~.-datetime-atemp, train_data)\nsummary(lm_bike)\n\n\nCall:\nlm(formula = count ~ . - datetime - atemp, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-297.02  -79.11  -25.60   46.35  585.12 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26.97221    8.45456   3.190  0.00143 ** \nseason2       9.25257    4.59790   2.012  0.04422 *  \nseason3     -45.22396    6.37114  -7.098 1.38e-12 ***\nseason4      22.00064    4.40455   4.995 6.02e-07 ***\nholiday1    -11.34365    8.77778  -1.292  0.19629    \nworkingday1  -4.86349    3.12625  -1.556  0.11982    \nweather2      3.02977    3.47616   0.872  0.38346    \nweather3    -32.19037    5.62008  -5.728 1.06e-08 ***\nweather4     88.33390  122.76150   0.720  0.47182    \ntemp          8.34447    0.29736  28.062  < 2e-16 ***\nhumidity     -1.47674    0.08879 -16.632  < 2e-16 ***\nwindspeed     0.06683    0.17945   0.372  0.70959    \nhour_rent     6.20731    0.21665  28.652  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.7 on 7608 degrees of freedom\nMultiple R-squared:  0.3422,    Adjusted R-squared:  0.3412 \nF-statistic: 329.9 on 12 and 7608 DF,  p-value: < 2.2e-16\n\n\nДалі побудуємо модель на основі алгоритму stepwise та порівняємо з поточними характеристиками моделі.\nСтворимо моделі для проходів у два боки:\n\nstart_mod <- lm(count ~ 1, data = train_data)\nend_mod <- lm(count ~ . - datetime - temp, data = train_data)\n\nЗапустимо алгоритм функцією step():\n\nlm_bike_stepwise <- step(start_mod,\n                     scope = list(lower = start_mod, upper = end_mod),\n                     direction = \"both\", trace = 1, steps = 1000)\n\nStart:  AIC=76488.12\ncount ~ 1\n\n             Df Sum of Sq       RSS   AIC\n+ atemp       1  28114587 145942663 75148\n+ hour_rent   1  27600736 146456514 75174\n+ humidity    1  15503642 158553609 75779\n+ season      3   9131936 164925314 76083\n+ weather     3   3537078 170520172 76338\n+ windspeed   1   2112392 171944858 76397\n<none>                    174057250 76488\n+ holiday     1     12170 174045081 76490\n+ workingday  1      3511 174053739 76490\n\nStep:  AIC=75147.53\ncount ~ atemp\n\n             Df Sum of Sq       RSS   AIC\n+ hour_rent   1  20603748 125338915 73990\n+ humidity    1  15576942 130365721 74289\n+ season      3   5178271 140764392 74878\n+ weather     3   2989985 142952678 74996\n+ windspeed   1   2889374 143053289 74997\n<none>                    145942663 75148\n+ workingday  1     34765 145907898 75148\n+ holiday     1        44 145942619 75150\n- atemp       1  28114587 174057250 76488\n\nStep:  AIC=73989.67\ncount ~ atemp + hour_rent\n\n             Df Sum of Sq       RSS   AIC\n+ humidity    1   8093815 117245100 73483\n+ season      3   2905720 122433195 73817\n+ weather     3   2861080 122477834 73820\n+ windspeed   1   1038961 124299954 73928\n<none>                    125338915 73990\n+ workingday  1     29661 125309254 73990\n+ holiday     1       313 125338602 73992\n- hour_rent   1  20603748 145942663 75148\n- atemp       1  21117599 146456514 75174\n\nStep:  AIC=73482.93\ncount ~ atemp + hour_rent + humidity\n\n             Df Sum of Sq       RSS   AIC\n+ season      3   2375796 114869304 73333\n+ weather     3    534048 116711052 73454\n+ workingday  1     32372 117212728 73483\n<none>                    117245100 73483\n+ windspeed   1     22790 117222310 73483\n+ holiday     1      6750 117238350 73484\n- humidity    1   8093815 125338915 73990\n- hour_rent   1  13120621 130365721 74289\n- atemp       1  22173444 139418544 74801\n\nStep:  AIC=73332.92\ncount ~ atemp + hour_rent + humidity + season\n\n             Df Sum of Sq       RSS   AIC\n+ weather     3    525600 114343704 73304\n+ workingday  1     42319 114826985 73332\n+ windspeed   1     31234 114838071 73333\n<none>                    114869304 73333\n+ holiday     1      2238 114867066 73335\n- season      3   2375796 117245100 73483\n- humidity    1   7563890 122433195 73817\n- atemp       1  12150238 127019542 74097\n- hour_rent   1  12266423 127135727 74104\n\nStep:  AIC=73303.97\ncount ~ atemp + hour_rent + humidity + season + weather\n\n             Df Sum of Sq       RSS   AIC\n+ windspeed   1     77129 114266576 73301\n+ workingday  1     30192 114313512 73304\n<none>                    114343704 73304\n+ holiday     1      3786 114339918 73306\n- weather     3    525600 114869304 73333\n- season      3   2367348 116711052 73454\n- humidity    1   5224113 119567817 73642\n- atemp       1  12055477 126399182 74066\n- hour_rent   1  12568342 126912046 74097\n\nStep:  AIC=73300.82\ncount ~ atemp + hour_rent + humidity + season + weather + windspeed\n\n             Df Sum of Sq       RSS   AIC\n<none>                    114266576 73301\n+ workingday  1     28599 114237977 73301\n+ holiday     1      4334 114262242 73303\n- windspeed   1     77129 114343704 73304\n- weather     3    571495 114838071 73333\n- season      3   2382626 116649202 73452\n- humidity    1   4315759 118582335 73581\n- atemp       1  12116310 126382885 74067\n- hour_rent   1  12411506 126678081 74085\n\n\n\n# check the summary\nsummary(lm_bike_stepwise)\n\n\nCall:\nlm(formula = count ~ atemp + hour_rent + humidity + season + \n    weather + windspeed, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-285.56  -78.70  -25.63   47.20  586.14 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  10.56655    8.35898   1.264   0.2062    \natemp         7.46224    0.26269  28.407  < 2e-16 ***\nhour_rent     6.21766    0.21626  28.750  < 2e-16 ***\nhumidity     -1.50051    0.08851 -16.954  < 2e-16 ***\nseason2       9.95637    4.55753   2.185   0.0289 *  \nseason3     -38.55100    6.14342  -6.275 3.68e-10 ***\nseason4      21.22676    4.40003   4.824 1.43e-06 ***\nweather2      2.68041    3.47136   0.772   0.4400    \nweather3    -31.47986    5.60817  -5.613 2.06e-08 ***\nweather4     83.31112  122.62122   0.679   0.4969    \nwindspeed     0.40641    0.17932   2.266   0.0235 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 7610 degrees of freedom\nMultiple R-squared:  0.3435,    Adjusted R-squared:  0.3426 \nF-statistic: 398.2 on 10 and 7610 DF,  p-value: < 2.2e-16\n\n\n\\(R^2\\) increses!\nОб’єднаємо для наочності коефіцієнти у один датафрейм:\n\nlm_coefs_def <- data.frame(lm_name = names(lm_bike$coefficients),\n                           name = lm_bike$coefficients, \n                           row.names = c() # cleaning rownames\n                          )\nlm_coefs_def\n\n\n\nA data.frame: 13 × 2\n\n    lm_namename\n    <chr><dbl>\n\n\n    (Intercept) 26.97221255\n    season2      9.25256711\n    season3    -45.22395805\n    season4     22.00064078\n    holiday1   -11.34364536\n    workingday1 -4.86348665\n    weather2     3.02977126\n    weather3   -32.19036573\n    weather4    88.33390060\n    temp         8.34447012\n    humidity    -1.47674448\n    windspeed    0.06683245\n    hour_rent    6.20730753\n\n\n\n\n\nlm_coefs_sw <- data.frame(lm_name = names(lm_bike_stepwise$coefficients), name = lm_bike_stepwise$coefficients, row.names = c())\nlm_coefs_sw\n\n\n\nA data.frame: 11 × 2\n\n    lm_namename\n    <chr><dbl>\n\n\n    (Intercept) 10.5665521\n    atemp        7.4622378\n    hour_rent    6.2176579\n    humidity    -1.5005074\n    season2      9.9563690\n    season3    -38.5510028\n    season4     21.2267554\n    weather2     2.6804117\n    weather3   -31.4798561\n    weather4    83.3111193\n    windspeed    0.4064147\n\n\n\n\n\n# lets combine coefficients from both models\nlm_coefs <- merge(lm_coefs_def, lm_coefs_sw, by = \"lm_name\", X.all = T, X.all = T)\ncolnames(lm_coefs) <- c(\"variable\", \"lm\", \"lm_sw\")\nlm_coefs\n\n\n\nA data.frame: 10 × 3\n\n    variablelmlm_sw\n    <chr><dbl><dbl>\n\n\n    (Intercept) 26.97221255 10.5665521\n    hour_rent    6.20730753  6.2176579\n    humidity    -1.47674448 -1.5005074\n    season2      9.25256711  9.9563690\n    season3    -45.22395805-38.5510028\n    season4     22.00064078 21.2267554\n    weather2     3.02977126  2.6804117\n    weather3   -32.19036573-31.4798561\n    weather4    88.33390060 83.3111193\n    windspeed    0.06683245  0.4064147\n\n\n\n\nЯк бачимо моделі мають незначні відмінності."
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#model-errors-analysis",
    "href": "032-supervised-learning-linear-regression-2.html#model-errors-analysis",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.6 Model errors analysis",
    "text": "3.6 Model errors analysis\nЗапишемо у train набір даних модельовані значення count та похибки:\n\ntrain_data$predicted <- lm_bike$fitted.values\ntrain_data$residuals <- lm_bike$residuals\nhead(train_data)\n\n\n\nA data.frame: 6 × 13\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rentpredictedresiduals\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl><dbl><dbl>\n\n\n    12011-01-01 00:00:0010019.8414.395810.0000160-10.534505 26.534505\n    22011-01-01 01:00:0010019.0213.635800.0000401 -9.692918 49.692918\n    32011-01-01 02:00:0010019.0213.635800.0000322 -3.485611 35.485611\n    42011-01-01 03:00:0010019.8414.395750.0000133 16.947885 -3.947885\n    52011-01-01 04:00:0010019.8414.395750.0000 14 23.155192-22.155192\n    62011-01-01 05:00:0010029.8412.880756.0032 15 32.793480-31.793480\n\n\n\n\nВізуалізуємо відхилення моделі:\n\nplot_data <- train_data[1:100,]\n\n\nggplot(plot_data, aes(x = c(1:nrow(plot_data)), y = as.numeric(count))) +\n  geom_segment(aes(xend = c(1:nrow(plot_data)), yend = predicted), alpha = .2) +\n  geom_point(aes(color = residuals), size = 2) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 1, size = 2) +\n  geom_hline(yintercept=0.5, linetype=\"dashed\", color = \"red\") +\n  theme_bw()\n\nWarning message:\n\"`guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\"\n\n\n\n\n\nLets check the same for test data\nFirst we need to predict values:\n\ntest_data$predicted <- predict(lm_bike, newdata = test_data)\nhead(test_data)\n\n\n\nA data.frame: 6 × 12\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rentpredicted\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl><dbl>\n\n\n    12012-05-16 22:00:00201126.2430.3057311.001428622279.8139\n    22012-05-16 23:00:00201125.4229.54578 7.001513323271.5277\n    32012-05-17 00:00:00201124.6028.79078 8.9981 79 0122.0506\n    42012-05-17 01:00:00201124.6028.0308311.0014 28 1121.0080\n    52012-05-17 02:00:00201124.6028.7907811.0014 16 2134.5991\n    62012-05-17 03:00:00201124.6029.5457316.9979  3 3148.5908\n\n\n\n\n\n# resulduals calculation\ntest_data$residuals <- test_data$count - test_data$predicted\nhead(test_data)\n\n\n\nA data.frame: 6 × 13\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rentpredictedresiduals\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl><dbl><dbl>\n\n\n    12012-05-16 22:00:00201126.2430.3057311.001428622279.8139   6.186142\n    22012-05-16 23:00:00201125.4229.54578 7.001513323271.5277-138.527654\n    32012-05-17 00:00:00201124.6028.79078 8.9981 79 0122.0506 -43.050553\n    42012-05-17 01:00:00201124.6028.0308311.0014 28 1121.0080 -93.008024\n    52012-05-17 02:00:00201124.6028.7907811.0014 16 2134.5991-118.599054\n    62012-05-17 03:00:00201124.6029.5457316.9979  3 3148.5908-145.590844\n\n\n\n\n\nplot_data <- test_data[1:100,]\n\n\nggplot(plot_data, aes(x = c(1:nrow(plot_data)), y = as.numeric(count))) +\n  geom_segment(aes(xend = c(1:nrow(plot_data)), yend = predicted), alpha = .2) +\n  geom_point(aes(color = residuals), size = 2) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 1, size = 2) +\n  geom_hline(yintercept=0.5, linetype=\"dashed\", color = \"red\") +\n  theme_bw()\n\nWarning message:\n\"`guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\"\n\n\n\n\n\n\nlibrary(modelr)\nresids <- data.frame(\n  R2 = c(rsquare(lm_bike, data = train_data), rsquare(lm_bike, data = test_data)),\n  MSE = c(mse(lm_bike, data = train_data),mse(lm_bike, data = test_data)),\n  RMSE = c(rmse(lm_bike, data = train_data), rmse(lm_bike, data = test_data))#,\n  #MAE = c(mae(lm_bike, data = train_data), mae(lm_bike, data = test_data)), # if needed \n  #MAPE = c(mape(lm_bike, data = train_data), mape(lm_bike, data = test_data)) # if needed\n)\n          \nrownames(resids) <- c(\"train\", \"test\") # give names for rows\n\nresids\n\n\n\nA data.frame: 2 × 3\n\n    R2MSERMSE\n    <dbl><dbl><dbl>\n\n\n    train0.342244315022.59122.5667\n    test0.285474140401.94201.0023\n\n\n\n\nConclusion: Test set has less RSquared and Bigger Errors variation. Train dataset is closer, because it built on training data and model include information about data."
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html",
    "href": "033-supervised-learning-desicion-trees-regression.html",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "",
    "text": "Прикладне математичне моделювання в R\nУ даній частині навчального процесу потрібно побудувати математичні моделі регресії клієнтів на основі алгоритму дерева рішень та перевірити їх на тестовій вибірці."
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#підготовка",
    "href": "033-supervised-learning-desicion-trees-regression.html#підготовка",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.1 Підготовка",
    "text": "4.1 Підготовка\n\n# install.packages(\"DMwR\")\n# install.packages(\"gmodels\")\n# install.packages(\"rpart\")\n# install.packages(\"partykit\")\n# install.packages(\"rpart.plot\")\n# install.packages(\"RColorBrewer\")\n# install.packages(\"rattle\")\n\n\nSys.setlocale(\"LC_CTYPE\", \"ukrainian\") \noptions(warn = -1)\n\n'Ukrainian_Ukraine.1251'\n\n\nДжерела:\n\nAn Introduction to Statistical Learning with Applications in R http://www-bcf.usc.edu/~gareth/ISL/data.html\nPredicting Credit Card Balance using Regression https://www.kaggle.com/suzanaiacob/predicting-credit-card-balance-using-regression/notebook\n\nДля виконнання завдань потрібний ряд R-пакетів:\n\nsuppressMessages(library(ggplot2)) # побудова графіків\nsuppressMessages(library(gmodels)) # побудова крос-таблиць\nsuppressMessages(library(rpart)) # дерево рішень\nsuppressMessages(library(partykit)) # дерево рішень\nsuppressMessages(library(rpart.plot)) # візуалізація дерева рішень\nsuppressMessages(library(DMwR)) # оцінка похибок моделі\nsuppressMessages(library(rattle)) # допомога у візуалізації \nsuppressMessages(library(RColorBrewer)) # допомога у візуалізації \n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\nЯкщо ці пакети відсутні, то інсталюйте їх за допомогою команди install.packages(назва_пакету).\nДля очистки сесії від непотрібних даних використайте команду rm():\nrm(list = ls()) #видаляє усі змінні"
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#набір-даних",
    "href": "033-supervised-learning-desicion-trees-regression.html#набір-даних",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.2 Набір даних",
    "text": "4.2 Набір даних\nДані складають з 400 спостежень та наступник пкоазників:\n\nID - ідентифікатор;\nIncome - дохід у $10,0000;\nLimit - кредитний ліміт;\nRating - кредитний рейтинг;\nAge - вік, роки;\nEducation - освіта, кількість років навчання;\nGender - стать (Male or Female);\nStudent - флаг чи є студентом (Yes or No);\nMarried - флаг чи одружений (Yes or No);\nEthnicity - етнічна належність (African American, Asian or Caucasian);\nBalance - середній баланс по карті у $$.\n\nЗадача: визначити вплив факторів на середній баланс по карті.\nІмпорт даних:\n\ndata <- read.csv(\"data/credit_card_balance.csv\")\n\nПереглянемо структуру даних:\n\nstr(data)\n\n'data.frame':   400 obs. of  12 variables:\n $ X        : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Income   : num  14.9 106 104.6 148.9 55.9 ...\n $ Limit    : int  3606 6645 7075 9504 4897 8047 3388 7114 3300 6819 ...\n $ Rating   : int  283 483 514 681 357 569 259 512 266 491 ...\n $ Cards    : int  2 3 4 3 2 4 2 2 5 3 ...\n $ Age      : int  34 82 71 36 68 77 37 87 66 41 ...\n $ Education: int  11 15 11 11 16 10 12 9 13 19 ...\n $ Gender   : chr  \"Male\" \"Female\" \"Male\" \"Female\" ...\n $ Student  : chr  \"No\" \"Yes\" \"No\" \"No\" ...\n $ Married  : chr  \"Yes\" \"Yes\" \"No\" \"No\" ...\n $ Ethnicity: chr  \"Caucasian\" \"Asian\" \"Asian\" \"Asian\" ...\n $ Balance  : int  333 903 580 964 331 1151 203 872 279 1350 ...\n\n\n\nhead(data)\n\n\n\nA data.frame: 6 × 12\n\n    XIncomeLimitRatingCardsAgeEducationGenderStudentMarriedEthnicityBalance\n    <int><dbl><int><int><int><int><int><chr><chr><chr><chr><int>\n\n\n    11 14.891360628323411Male  No YesCaucasian 333\n    22106.025664548338215FemaleYesYesAsian     903\n    33104.593707551447111Male  No No Asian     580\n    44148.924950468133611FemaleNo No Asian     964\n    55 55.882489735726816Male  No YesCaucasian 331\n    66 80.180804756947710Male  No No Caucasian1151\n\n\n\n\nОглянемо описову статистику факторів:\n\nsummary(data)\n\n       X             Income           Limit           Rating     \n Min.   :  1.0   Min.   : 10.35   Min.   :  855   Min.   : 93.0  \n 1st Qu.:100.8   1st Qu.: 21.01   1st Qu.: 3088   1st Qu.:247.2  \n Median :200.5   Median : 33.12   Median : 4622   Median :344.0  \n Mean   :200.5   Mean   : 45.22   Mean   : 4736   Mean   :354.9  \n 3rd Qu.:300.2   3rd Qu.: 57.47   3rd Qu.: 5873   3rd Qu.:437.2  \n Max.   :400.0   Max.   :186.63   Max.   :13913   Max.   :982.0  \n     Cards            Age          Education        Gender         \n Min.   :1.000   Min.   :23.00   Min.   : 5.00   Length:400        \n 1st Qu.:2.000   1st Qu.:41.75   1st Qu.:11.00   Class :character  \n Median :3.000   Median :56.00   Median :14.00   Mode  :character  \n Mean   :2.958   Mean   :55.67   Mean   :13.45                     \n 3rd Qu.:4.000   3rd Qu.:70.00   3rd Qu.:16.00                     \n Max.   :9.000   Max.   :98.00   Max.   :20.00                     \n   Student            Married           Ethnicity            Balance       \n Length:400         Length:400         Length:400         Min.   :   0.00  \n Class :character   Class :character   Class :character   1st Qu.:  68.75  \n Mode  :character   Mode  :character   Mode  :character   Median : 459.50  \n                                                          Mean   : 520.01  \n                                                          3rd Qu.: 863.00  \n                                                          Max.   :1999.00  \n\n\nПідготуємо дані до моделювання. Перетворимо категоріальні показники до факторів:\n\ndata$X <- NULL\ndata$Gender <-  factor(data$Gender)\ndata$Student <- factor(data$Student)\ndata$Married <- factor(data$Married)\ndata$Ethnicity <- factor(data$Ethnicity)\n\n\n\n\n\n\n\nNote\n\n\n\nСпростіть код, використавши можливості пакету dplyr."
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#тренувальна-та-тестова-вибірки",
    "href": "033-supervised-learning-desicion-trees-regression.html#тренувальна-та-тестова-вибірки",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.3 Тренувальна та тестова вибірки",
    "text": "4.3 Тренувальна та тестова вибірки\nРозділимо загальну вибірку на 2 частини: * тренувальна, 70% вибірки, для побудови регресії; * тестова, 30% вибірки, для перевірки точності моделі.\n\nset.seed(2023) #довільне число як точка \"відправки\" для генератора випадкових чисел\n\n# Згенеруємо набір чисел від 1 до кількості спостережень у вибірці і відберемо  випадквоим чином 70% із них\ntrain_index <- sample(1:nrow(data), size = 0.7*nrow(data))\n\n#Запишемо по номерах відібраних рядків тренувальний набір даних\ntrain_data <- data[train_index,]\n\n#Всі інші значення, що не увійшли в тренувальну вибірку запишемо у тестову\ntest_data <- data[-train_index,]\n\nПереглянемо наявність зв’язків між числовими параметрами для тренувальної вибірки за допомогою матриці попарних кореляцій/ Дані на перетині рядків вказують на кореляцію між вибраними показниками.\n\ncor(train_data[, -c(7:10)])\n\n\n\nA matrix: 7 × 7 of type dbl\n\n    IncomeLimitRatingCardsAgeEducationBalance\n\n\n    Income1.000000000.797412690.79561996 0.017137980.14965311 0.001772920.49497408\n    Limit0.797412691.000000000.99694968 0.045597840.10507642 0.016592740.87729947\n    Rating0.795619960.996949681.00000000 0.089412760.10494558 0.012338530.87873569\n    Cards0.017137980.045597840.08941276 1.000000000.03596854-0.051421400.09550304\n    Age0.149653110.105076420.10494558 0.035968541.00000000 0.071655030.03652931\n    Education0.001772920.016592740.01233853-0.051421400.07165503 1.000000000.04109140\n    Balance0.494974080.877299470.87873569 0.095503040.03652931 0.041091401.00000000"
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#оглядовий-аналіз-даних",
    "href": "033-supervised-learning-desicion-trees-regression.html#оглядовий-аналіз-даних",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.4 Оглядовий аналіз даних",
    "text": "4.4 Оглядовий аналіз даних\nДля початку переглянемо категоріальні змінні.\n\n# Потбірні пакети\nlibrary(ggplot2)\nlibrary(gmodels)\n\nСтать (Gender):\n\nggplot(train_data, aes(Gender)) + \n    geom_bar(aes(fill = Gender)) +\n    theme_bw()\n\n\n\n\n\nCrossTable(train_data$Gender)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  280 \n\n \n          |    Female |      Male | \n          |-----------|-----------|\n          |       149 |       131 | \n          |     0.532 |     0.468 | \n          |-----------|-----------|\n\n\n\n \n\n\nСімейний стан (Married):\n\nggplot(train_data, aes(Married)) + geom_bar(aes(fill = Married)) + theme_bw()\n\n\n\n\n\nCrossTable(train_data$Married)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  280 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |       101 |       179 | \n          |     0.361 |     0.639 | \n          |-----------|-----------|\n\n\n\n \n\n\nСтудент:\n\nggplot(train_data, aes(Student)) + geom_bar(aes(fill = Student)) + theme_bw()\n\n\n\n\n\nCrossTable(train_data$Student)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  280 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |       255 |        25 | \n          |     0.911 |     0.089 | \n          |-----------|-----------|\n\n\n\n \n\n\nЕтнічна належність:\n\nggplot(train_data, aes(Ethnicity)) + geom_bar(aes(fill = Ethnicity)) + theme_bw()\n\n\n\n\n\nCrossTable(train_data$Ethnicity)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  280 \n\n \n                 | African American |            Asian |        Caucasian | \n                 |------------------|------------------|------------------|\n                 |               67 |               65 |              148 | \n                 |            0.239 |            0.232 |            0.529 | \n                 |------------------|------------------|------------------|\n\n\n\n \n\n\nПорівняємо числові змінні з показником середнього балансу.\nГрафік залежності між доходом (Income) та середнім балансом по карті (Balance):\n\nggplot(train_data, aes(Income, Balance)) + geom_point()  + theme_bw()\n\n\n\n\nГрафік залежності між доходом (Rating) та середнім балансом по карті (Balance):\n\nggplot(train_data, aes(Rating, Balance)) + geom_point()  + theme_bw()\n\n\n\n\nГрафік залежності між кількістю карт (Cards) та середнім балансом по карті (Balance):\n\nggplot(train_data, aes(Cards, Balance)) + geom_point()  + theme_bw()\n\n\n\n\nГрафік залежності між віком (Age) та середнім балансом по карті (Balance):\n\nggplot(train_data, aes(Education, Balance)) + geom_point()  + theme_bw()\n\n\n\n\nГрафік розподілу значень балансу:"
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#моделювання.-робота-з-пакетом-rpart",
    "href": "033-supervised-learning-desicion-trees-regression.html#моделювання.-робота-з-пакетом-rpart",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.5 Моделювання. Робота з пакетом RPart",
    "text": "4.5 Моделювання. Робота з пакетом RPart\n\n4.5.1 Побудова моделі без налаштувань\nДля побудови регресії на основі дерева рішень інсталюємо пакет rpart (install.package(\"rpart\")).\nВикористаємо функцію для побудови моделі rpart():\n\n#library(rpart)\nrpart_model <- rpart(Balance ~ ., train_data)\n\nСтворимо дата-фрейми для запису результатів моделювання на тестовій та тренувальній вибірках:\n\ntrain_res <- data.frame(No = c(1:nrow(train_data)), \n                        Balance = train_data$Balance, \n                        RPartPredictedDef = predict(rpart_model, train_data))\n\ntest_res <- data.frame(No = c(1:nrow(test_data)),\n                       Balance = test_data$Balance, \n                       RPartPredictedDef = predict(rpart_model, test_data))\n\nhead(train_res)\n\n\n\nA data.frame: 6 × 3\n\n    NoBalanceRPartPredictedDef\n    <int><int><dbl>\n\n\n    3731840453.17778\n    3612712891.97561\n    2433 16 64.26136\n    2824  0 64.26136\n    445976891.97561\n    3546425453.17778\n\n\n\n\n\n\n4.5.2 Оцінка метрик\nПереглянемо похибки моделі на обох вибірках:\n\n#library(DMwR)\nregr.eval(train_res$Balance, train_res$RPartPredictedDef)\nregr.eval(test_res$Balance, test_res$RPartPredictedDef)\n\nmae119.689257298022mse26183.8847433001rmse161.814352711062mapeInf\n\n\nmae159.674646317362mse49034.8928845991rmse221.438237178223mapeInf\n\n\nОцінимо точність моделі за допомогою коефіцієнта детермінації:\n\nr_train <- cor(train_res$Balance, train_res$RPartPredictedDef)^2\nr_test <- cor(test_res$Balance, test_res$RPartPredictedDef)^2\nprint(paste(\"R_train = \", r_train, sep = \"\"))\nprint(paste(\"R_test = \", r_test, sep = \"\"))\n\n[1] \"R_train = 0.872679719680806\"\n[1] \"R_test = 0.780363587175022\"\n\n\nДля покращення візуалізації відсортуємо результати:\n\n# відсортуємо за зростанням значення балансів карт\nordered_train_res <- train_res[order(train_res$Balance),]\n# відсортуємо за зростанням значення модельованого значення балансу\nordered_train_res <- train_res[order(train_res$RPartPredictedDef),]\n# \"Перепишемо\" номери по порядку\nordered_train_res$No <- c(1:nrow(train_res))\n\nПобудуємо графік модельованих та реальних значень балансу з відсортованими показниками для наочності:\n\nggplot(ordered_train_res) +\n  geom_point(aes(x = No, y = Balance), colour = \"blue\") +\n  geom_line(aes(x = No, y = RPartPredictedDef), colour = \"red\", size = 1) + theme_bw()\n\n\n\n\nПереглянемо залежність між рейтингом та балансом клієнта, а також прогнозованих значеннях.\n\nggplot(train_data) +\n  geom_point(aes(x = Rating, y = Balance), colour = \"blue\") +\n  geom_line(aes(x = Rating, y = train_res$RPartPredictedDef), colour = \"red\") + theme_bw()\n\n\n\n\n\n\n4.5.3 Візуальне представлення\nДля візуалізації дерева рішень скористаємося пакетом rpart.plot:\n\n#library(rpart.plot)\nprp(rpart_model)\n\n\n\n\n\nprp(rpart_model, extra = 1, type = 2)\n\n\n\n\nДодамо інтерактивності для побудованого дерева рішень:\n\nprp(rpart_model, snip = TRUE) # Працює у RStudio\n\n\n\n\nПобудуємо також “розфарбоване” дерево рішень за допомогою пакетів rattle та RColorBrewer:\n\n#library(rattle)\n#library(RColorBrewer)\nfancyRpartPlot(rpart_model)\n\n\n\n\n\n\n\n4.5.4 Побудова моделі з розділеннями\nПобудуємо модель з вказанням мінімальної кількості розділень даних (minsplit):\n\nrpart_model2 <- rpart(Balance ~ ., train_data, control = rpart.control(minsplit = 10))\n\nОтримаємо прогнозовані значення для обох вибірок на основі другої моделі:\n\ntrain_res$RPartPredicted10 <- predict(rpart_model2, train_data)\ntest_res$RPartPredicted10 <- predict(rpart_model2, test_data)\n\nhead(train_res)\n\n\n\nA data.frame: 6 × 4\n\n    NoBalanceRPartPredictedDefRPartPredicted10\n    <int><int><dbl><dbl>\n\n\n    3731840453.17778836.25000\n    3612712891.97561891.97561\n    2433 16 64.26136 64.26136\n    2824  0 64.26136 64.26136\n    445976891.97561891.97561\n    3546425453.17778415.80488\n\n\n\n\n\n\n4.5.5 Оцінка метрик\nПереглянемо похибки:\n\nregr.eval(train_res$Balance, train_res$RPartPredicted10)\nregr.eval(test_res$Balance, test_res$RPartPredicted10)\n\nmae115.002097019276mse23883.0154977523rmse154.541306768619mapeInf\n\n\nmae152.524889090623mse43219.8819877103rmse207.89392003546mapeInf\n\n\nОцінимо точність моделі за допомогою коефіцієнта детермінації:\n\nr_train <- cor(train_res$Balance, train_res$RPartPredicted10)^2\nr_test <- cor(test_res$Balance, test_res$RPartPredicted10)^2\n\nprint(paste(\"R_train =\", r_train, sep = \"\"))\nprint(paste(\"R_test =\", r_test, sep = \"\"))\n\n[1] \"R_train =0.883867796629392\"\n[1] \"R_test =0.805983358303455\"\n\n\n\n\n4.5.6 Візуалізація результатів\nВізуалізуємо модель:\n\nprp(rpart_model2)"
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#моделювання.-робота-з-пакетом-partykit",
    "href": "033-supervised-learning-desicion-trees-regression.html#моделювання.-робота-з-пакетом-partykit",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.6 Моделювання. Робота з пакетом partykit",
    "text": "4.6 Моделювання. Робота з пакетом partykit\nПобудуємо дерево рішень на основі partykit та функції ctree() та порівняємо з результатами роботи rpart().\n\n#library(partykit)\nparty_model <- ctree(Balance ~ ., data = train_data[1:100,]) # перших 100 спостережень\nprint(party_model)\n\n\nModel formula:\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student + Married + Ethnicity\n\nFitted party:\n[1] root\n|   [2] Rating <= 342\n|   |   [3] Rating <= 245: 3.360 (n = 25, err = 4597.8)\n|   |   [4] Rating > 245\n|   |   |   [5] Income <= 19.782: 416.200 (n = 10, err = 290573.6)\n|   |   |   [6] Income > 19.782: 221.889 (n = 18, err = 254079.8)\n|   [7] Rating > 342\n|   |   [8] Rating <= 599: 765.051 (n = 39, err = 1913599.9)\n|   |   [9] Rating > 599: 1380.625 (n = 8, err = 720389.9)\n\nNumber of inner nodes:    4\nNumber of terminal nodes: 5\n\n\n\nplot(party_model)\n\n\n\n\n\nparty_model <- ctree(Balance ~ ., data = train_data) \nprint(party_model)\n\n\nModel formula:\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student + Married + Ethnicity\n\nFitted party:\n[1] root\n|   [2] Rating <= 353\n|   |   [3] Rating <= 278\n|   |   |   [4] Student in No\n|   |   |   |   [5] Rating <= 249: 8.516 (n = 62, err = 64369.5)\n|   |   |   |   [6] Rating > 249: 167.056 (n = 18, err = 91486.9)\n|   |   |   [7] Student in Yes: 265.000 (n = 8, err = 223014.0)\n|   |   [8] Rating > 278\n|   |   |   [9] Income <= 44.978: 453.178 (n = 45, err = 1330364.6)\n|   |   |   [10] Income > 44.978: 104.273 (n = 11, err = 45162.2)\n|   [11] Rating > 353\n|   |   [12] Rating <= 682\n|   |   |   [13] Limit <= 5310\n|   |   |   |   [14] Income <= 48.218: 717.778 (n = 27, err = 670882.7)\n|   |   |   |   [15] Income > 48.218: 352.583 (n = 12, err = 243564.9)\n|   |   |   [16] Limit > 5310\n|   |   |   |   [17] Student in No\n|   |   |   |   |   [18] Income <= 101.788\n|   |   |   |   |   |   [19] Rating <= 536\n|   |   |   |   |   |   |   [20] Income <= 63.809\n|   |   |   |   |   |   |   |   [21] Rating <= 456\n|   |   |   |   |   |   |   |   |   [22] Income <= 34.48: 934.222 (n = 18, err = 95773.1)\n|   |   |   |   |   |   |   |   |   [23] Income > 34.48: 808.000 (n = 7, err = 45006.0)\n|   |   |   |   |   |   |   |   [24] Rating > 456: 1028.533 (n = 15, err = 106875.7)\n|   |   |   |   |   |   |   [25] Income > 63.809: 734.714 (n = 14, err = 332820.9)\n|   |   |   |   |   |   [26] Rating > 536: 1173.250 (n = 12, err = 307664.2)\n|   |   |   |   |   [27] Income > 101.788: 667.545 (n = 11, err = 816958.7)\n|   |   |   |   [28] Student in Yes: 1160.778 (n = 9, err = 523847.6)\n|   |   [29] Rating > 682: 1584.909 (n = 11, err = 510442.9)\n\nNumber of inner nodes:    14\nNumber of terminal nodes: 15\n\n\n\nplot(party_model)\n\n\n\n\nОбчислимо модельовані значення середнього балансу по карті:\n\ntrain_res$PredictedPartyDef <- predict(party_model, train_data)\ntest_res$PredictedPartyDef <- predict(party_model, test_data)\n\nПереглянемо коефіцієнти детермінації:\n\nr_train <- cor(train_res$Balance, train_res$PredictedPartyDef)^2\nr_test <- cor(test_res$Balance, test_res$PredictedPartyDef)^2\n\nprint(paste(\"R_train = \", r_train, sep = \"\"))\nprint(paste(\"R_test = \", r_test, sep = \"\"))\n\n[1] \"R_train = 0.906079382030953\"\n[1] \"R_test = 0.857313458113989\""
  }
]